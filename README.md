# HadoopSparkProgressTree
Hadoop, Spark数据处理技术

<pre>
MapReduce是一个软件框架，可以采用并行，分布式方式处理GB,TB,PB级的大数据集，同时它也是一个在商用服务器集群之上完成大规模数据处理的执行框架。
</pre>

![](https://i.imgur.com/8qmivRo.jpg)

<pre>
MapReduce是一种编程范式，可以利用集群环境的成百上千台服务器实现强大的可伸缩性，
Map:过滤和聚集数据
    主节点得到输入，将输入划分为较小的数据块，再将这些小数据块分布到工作节点（从节点），工作节点对各个数据块应用相同的转换函数，然后再将结果传回到主节点 
Reduce:根据Map生成的键完成规约，分组和总结
    主节点根据唯一的键值对将接受到的结果进行洗牌和聚集；然后再一次重新分布到工作节点，通过另一类转换函数组合这些值。
所有映射器的工作完成时，这些键会经过排序，洗牌，分组，然后发送给规约器，最后规约器将生成所需的输出	 
</pre>

<pre>
MapReduce不适用的场景
     1）一个值的计算依赖与之前计算的值
	 2）数据集很小，完全可以再一台机器上计算
	 3）需要同步来处理共享数据
	 5）所有输入数据都可以放在内存张
	 6）一个操作依赖其他操作
	 7）基本计算是处理器敏感性操作
	 
MapReduce适用的场景
     1）必须处理大量输入数据
     2）需要利用并行分布式计算，数据存储和数据本地化
     3）可以独立地完成很多任务而无需同步
     5）可以利用排序和洗牌
     6）需要容错性，不接受作业失败	

MapReduce提供的优点
     1：编程模型 + 基础架构
     2：能够编写在数百甚至数千台机器上运行的程序
     3：自动并行化和分布
     5：容错
     6：程序/作业调度，状态检查和监控	
</pre>

<pre>
Hadoop与Spark
     Hadoop是一个MapReduce框架，在这个 框架上运行支持map(), combine(),reduce()函数的作业
     Spark不是一个MapReduce框架，
</pre>